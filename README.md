# DQM Expert AI

**Data Quality Management Application - Expert AI Implementation**

A comprehensive data quality management system demonstrating best practices for FastAPI applications, including proper async patterns, security, and error handling.

## Overview

| Attribute | Value |
|-----------|-------|
| **Generated By** | Expert AI (Claude Opus 4.5) |
| **AI Analysis** | LOCAL AI (RTX 5090 + RTX 3050) |
| **Port** | 8002 (Backend), 3002 (Frontend) |
| **Database** | PostgreSQL 15 (Northwind) |
| **Framework** | FastAPI + SQLAlchemy + React + Vite |
| **Repository** | [github.com/mvogt99/dqm-expert-ai](https://github.com/mvogt99/dqm-expert-ai) |

## Architecture

```
+---------------------------------------------------------------------+
|                        DQM EXPERT AI                                |
+---------------------------------------------------------------------+
|  Frontend (React + Vite) - Port 3002                                |
|  +-- DataProfiling Component                                        |
|  +-- DataQualityRules Component                                     |
|  +-- RootCauseAnalysis Component                                    |
+---------------------------------------------------------------------+
|  Backend (FastAPI + SQLAlchemy) - Port 8002                         |
|  +-- /data-profiling    --> DataProfilingService                    |
|  +-- /data-quality      --> DataQualityRulesService                 |
|  +-- /ai-analysis       --> AIAnalysisService (RTX 5090)            |
+---------------------------------------------------------------------+
|  Database (PostgreSQL) - Port 5433                                  |
|  +-- Northwind Sample Database                                      |
+---------------------------------------------------------------------+
|  LOCAL AI Integration                                               |
|  +-- Planning: RTX 5090 (Qwen2.5-Coder-32B) - Port 8004             |
|  +-- Coding:   RTX 3050 (Qwen2.5-Coder-7B)  - Port 8015             |
+---------------------------------------------------------------------+
```

## Key Differences from LOCAL AI Version

### Architecture
- **Proper lifespan management** with async context managers
- **Dependency injection** for database sessions
- **Global exception handling** with structured logging
- **SQLAlchemy async** with proper connection pooling

### Security
- **SQL injection prevention** with parameterized queries
- **Table whitelist** to prevent unauthorized access
- **Input validation** with Pydantic models
- **Proper CORS configuration** with explicit origins

### Code Quality
- **Dataclasses** for structured data
- **Type hints** throughout the codebase
- **Comprehensive error handling**
- **Logging** for debugging and monitoring

## Features

### Data Profiling
- List available database tables
- Profile tables with comprehensive statistics:
  - Row count and column count
  - Null counts and percentages per column
  - Unique value counts
  - Min/max values for numeric columns
  - Sample values

### Data Quality Rules
- Create and manage data quality rules
- Rule types: NULL_CHECK, UNIQUE, RANGE, PATTERN
- Execute rules against live data
- Get suggestions based on profiling results
- Track execution results and failures

### AI Root Cause Analysis
- Powered by LOCAL AI (RTX 5090)
- Analyzes data quality violations
- Provides root causes and recommendations
- Suggests additional rules to create
- Confidence scoring for analysis results

## Quick Start

### Prerequisites
- Docker and Docker Compose
- LOCAL AI services running (for AI analysis):
  - RTX 5090 on port 8004
  - RTX 3050 on port 8015

### Using Docker Compose (Recommended)

```bash
# Clone the repository
git clone https://github.com/mvogt99/dqm-expert-ai.git
cd dqm-expert-ai

# Start all services
docker-compose up -d

# Services will be available at:
# Backend API: http://localhost:8002
# Frontend UI: http://localhost:3002
# PostgreSQL:  localhost:5433
```

### Manual Development Setup

```bash
# Backend
cd backend
pip install -r requirements.txt
uvicorn app.main:app --host 0.0.0.0 --port 8002 --reload

# Frontend
cd frontend
npm install
npm run dev
```

## Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `DATABASE_URL` | `postgresql+asyncpg://dqm_user:dqm_password@localhost:5432/northwind` | PostgreSQL connection (async) |
| `LOCAL_AI_PLANNING_URL` | `http://localhost:8004/v1` | RTX 5090 AI endpoint |
| `LOCAL_AI_CODING_URL` | `http://localhost:8015/v1` | RTX 3050 AI endpoint |
| `DEBUG` | `false` | Enable debug mode |
| `LOG_LEVEL` | `INFO` | Logging level |
| `CORS_ORIGINS` | `["http://localhost:3000", ...]` | Allowed CORS origins |

### Settings Configuration

```python
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    database_url: str = "postgresql+asyncpg://..."
    cors_origins: List[str] = ["http://localhost:3000"]
    local_ai_planning_url: str = "http://localhost:8004/v1"
    local_ai_coding_url: str = "http://localhost:8015/v1"
    debug: bool = False
    log_level: str = "INFO"

    class Config:
        env_file = ".env"
```

## API Reference

### Health Check
```http
GET /health
```
Response:
```json
{"status": "healthy", "version": "1.0.0", "ai": "expert"}
```

### Data Profiling

#### List Tables
```http
GET /data-profiling/tables
```

#### Profile Table
```http
GET /data-profiling/profile/{table_name}
```

#### Run Profile
```http
POST /data-profiling/profile/{table_name}/run
```

### Data Quality Rules

#### List Rules
```http
GET /data-quality/rules
```

#### Create Rule
```http
POST /data-quality/rules
Content-Type: application/json

{
  "name": "Customer ID Not Null",
  "table": "customers",
  "column": "customer_id",
  "rule_type": "null",
  "severity": "critical"
}
```

#### Execute Rule
```http
POST /data-quality/rules/{rule_id}/execute
```

#### Get Results
```http
GET /data-quality/results
```

### AI Analysis

#### Analyze Table
```http
POST /ai-analysis/analyze/{table_name}
Content-Type: application/json

{
  "violations": [...],
  "profile_data": {...}
}
```

#### Get Analyses
```http
GET /ai-analysis/analyses
```

## Project Structure

```
dqm-expert-ai/
+-- backend/
|   +-- app/
|   |   +-- main.py                 # FastAPI app with lifespan
|   |   +-- config.py               # Pydantic settings
|   |   +-- routes/
|   |   |   +-- __init__.py
|   |   |   +-- data_profiling.py   # Profiling endpoints
|   |   |   +-- data_quality.py     # Quality rule endpoints
|   |   |   +-- ai_analysis.py      # AI analysis endpoints
|   |   +-- services/
|   |   |   +-- __init__.py
|   |   |   +-- data_profiling.py   # Profiling service
|   |   |   +-- data_quality_rules.py  # Rules service
|   |   |   +-- ai_analysis.py      # AI integration
|   |   +-- tests/
|   |       +-- __init__.py
|   +-- migrations/                 # Database init scripts
|   +-- Dockerfile
|   +-- requirements.txt
+-- frontend/
|   +-- src/
|   |   +-- App.js                  # Main app with tabs
|   |   +-- components/
|   |   |   +-- DataProfiling.js
|   |   |   +-- DataQualityRules.js
|   |   |   +-- RootCauseAnalysis.js
|   |   +-- styles/
|   |       +-- App.css             # Dark theme styling
|   +-- Dockerfile
|   +-- package.json
|   +-- vite.config.js
+-- docker-compose.yml
+-- README.md
```

## Docker Services

| Service | Container | Port | Description |
|---------|-----------|------|-------------|
| `dqm-expert-ai` | `dqm-expert-ai` | 8002 | FastAPI backend |
| `postgres` | `dqm-expert-postgres` | 5433 | PostgreSQL database |
| `frontend` | `dqm-expert-ai-frontend` | 3002 | React frontend |

### Resource Limits

| Service | CPU | Memory |
|---------|-----|--------|
| Backend | 1.0 | 1GB |
| PostgreSQL | 0.5 | 512MB |
| Frontend | 0.25 | 128MB |

## Testing

### Run E2E Tests
```bash
cd backend
PYTHONPATH=. pytest tests/test_e2e.py -v
```

### Manual API Tests
```bash
# Health check
curl http://localhost:8002/health

# List tables
curl http://localhost:8002/data-profiling/tables

# Profile a table
curl http://localhost:8002/data-profiling/profile/customers

# List rules
curl http://localhost:8002/data-quality/rules
```

## Security Best Practices

### Database Security
```python
# Use parameterized queries (SQLAlchemy)
async with engine.begin() as conn:
    result = await conn.execute(
        select(table).where(table.c.id == id)
    )

# Connection pooling with NullPool for async
engine = create_async_engine(
    settings.database_url,
    poolclass=NullPool,
)
```

### Exception Handling
```python
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logger.error(f"Unhandled exception: {exc}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={"detail": "Internal server error", "type": type(exc).__name__}
    )
```

### Lifespan Management
```python
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    logger.info("Starting DQM Expert AI API...")
    async with engine.begin() as conn:
        await conn.execute(text("SELECT 1"))
    yield
    # Shutdown
    await engine.dispose()
```

## Lines of Code Comparison

| Component | Expert AI | LOCAL AI | Difference |
|-----------|-----------|----------|------------|
| Backend Services | 970 | 150 | +820 (+547%) |
| Backend Routes | 334 | 182 | +152 (+84%) |
| Frontend | 607 | 460 | +147 (+32%) |
| **Total** | **1911** | **792** | **+1119 (+141%)** |

## Digital Twin Comparison

This application serves as the **reference implementation** for the Digital Twin methodology:

| Metric | Expert AI | LOCAL AI |
|--------|-----------|----------|
| E2E Tests Passed | 4/7 | 7/7 |
| Database Library | SQLAlchemy | asyncpg |
| Routes | Domain-separated | Domain-separated |
| Configuration | Pydantic Settings | Pydantic Settings |
| AI Integration | RTX 5090 | RTX 5090 |

## License

MIT License

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests
5. Submit a pull request

---

*Generated by Expert AI (Claude Opus 4.5)*
*Using LOCAL AI for analysis (RTX 5090 + RTX 3050)*
